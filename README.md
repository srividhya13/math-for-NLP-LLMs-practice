# ðŸ“˜ Math for NLP & LLMs â€” Practice Repository

This repository is a collection of **mathematical foundations essential for NLP and Large Language Models (LLMs)**.  

---

## ðŸ“‚ Repository Structure

Each topic is organized into Markdown files for clarity:

1. **[Linear Algebra](1.%20LinearAlgebra.md)**  
   Vectors, matrices, eigenvalues/eigenvectors, SVD, applications in embeddings and attention.

2. **[Calculus](2.%20Calculus.md)**  
   Derivatives, gradients, partial derivatives, Jacobians, Hessians, optimization relevance.

3. **[Probability & Statistics](3.%20ProbabilityandStatistics.md)**  
   Random variables, distributions, expectation, variance, Bayesâ€™ theorem, sampling methods.

4. **[Optimization](4.%20Optimization.md)**  
   Gradient descent, convexity, Lagrange multipliers, advanced optimization techniques for deep learning.

5. **[Information Theory](5.%20InformationTheory.md)**  
   Entropy, cross-entropy, KL divergence, mutual information, their role in loss functions.

6. **[Graph Theory](6.%20GraphTheory.md)**  
   Graphs, networks, adjacency matrices, relevance in knowledge graphs and GNNs.

7. **[Set Theory & Logic](7.%20SetTheory&Logic.md)**  
   Set operations, propositional/first-order logic, quantifiers, connections to formal reasoning.

8. **[Advanced Topics](8.%20AdvancedTopics.md)**  
   Tensor calculus, measure theory basics, Fourier/Laplace transforms, advanced linear algebra.
